# æºç åˆ†æ kubernetes scheduler æ ¸å¿ƒè°ƒåº¦å™¨çš„å®ç°åŸç†

> ä¸»è¦å†…å®¹è½¬è½½è‡ªhttps://github.com/rfyiamcool/notes

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301251502086.png)



> åŸºäº kubernetes `v1.27.0` æºç åˆ†æ scheduler è°ƒåº¦å™¨

k8s scheduler çš„ä¸»è¦èŒè´£æ˜¯ä¸ºæ–°åˆ›å»ºçš„ pod å¯»æ‰¾ä¸€ä¸ªæœ€åˆé€‚çš„ node èŠ‚ç‚¹, ç„¶åè¿›è¡Œ bind node ç»‘å®š, åé¢ kubelet æ‰ä¼šç›‘å¬åˆ°å¹¶åˆ›å»ºçœŸæ­£çš„ pod.

é‚£ä¹ˆé—®é¢˜æ¥äº†, å¦‚ä½•ä¸º pod å¯»æ‰¾æœ€åˆé€‚çš„ node ? è°ƒåº¦å™¨éœ€è¦ç»è¿‡ predicates é¢„é€‰å’Œ priority ä¼˜é€‰.

- é¢„é€‰å°±æ˜¯ä»é›†ç¾¤çš„æ‰€æœ‰èŠ‚ç‚¹ä¸­æ ¹æ®è°ƒåº¦ç®—æ³•ç­›é€‰å‡ºæ‰€æœ‰å¯ä»¥è¿è¡Œè¯¥ pod çš„èŠ‚ç‚¹é›†åˆ
- ä¼˜é€‰åˆ™æ˜¯æŒ‰ç…§ç®—æ³•å¯¹é¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œæ‰¾åˆ°åˆ†å€¼æœ€é«˜çš„èŠ‚ç‚¹ä½œä¸ºè°ƒåº¦èŠ‚ç‚¹.

é€‰å‡ºæœ€ä¼˜èŠ‚ç‚¹å, å¯¹ apiserver å‘èµ· pod èŠ‚ç‚¹ bind æ“ä½œ, å…¶å®å°±æ˜¯å¯¹ pod çš„ spec.NodeName èµ‹å€¼æœ€ä¼˜èŠ‚ç‚¹.

**æºç åŸºæœ¬è°ƒç”¨å…³ç³»**

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202212/k8s-scheduler.jpg)

## k8s scheduler å¯åŠ¨å…¥å£

k8s scheduler åœ¨å¯åŠ¨æ—¶ä¼šè°ƒç”¨åœ¨ cobra æ³¨å†Œçš„ `setup` æ¥åˆå§‹åŒ– scheduler è°ƒåº¦å™¨å¯¹è±¡.

ä»£ç ä½ç½®: `cmd/kube-scheduler/app/server.go`

```go
func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {
	// è·å–é»˜è®¤é…ç½®
	if cfg, err := latest.Default(); err != nil {
		return nil, nil, err
	} else {
		opts.ComponentConfig = cfg
	}

	// éªŒè¯ scheduler çš„é…ç½®å‚æ•°
	if errs := opts.Validate(); len(errs) > 0 {
		return nil, nil, utilerrors.NewAggregate(errs)
	}

	c, err := opts.Config()
	if err != nil {
		return nil, nil, err
	}

	// é…ç½®ä¸­å¡«å……å’Œè°ƒæ•´
	cc := c.Complete()

	...

	// æ„å»º scheduler å¯¹è±¡
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		...
	)
	if err != nil {
		return nil, nil, err
	}
	...

	return &cc, sched, nil
}
```

å®ä¾‹åŒ– kubernetes scheduler å¯¹è±¡, åˆå§‹åŒ–æµç¨‹ç›´æ¥çœ‹ä¸‹é¢ä»£ç .

ä»£ç ä½ç½®: `pkg/scheduler/scheduler.go`

```go
// New returns a Scheduler
func New(client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	stopCh <-chan struct{},
	opts ...Option) (*Scheduler, error) {


	// æ„å»º registry å¯¹è±¡, é»˜è®¤é›†æˆäº†ä¸€å †çš„æ’ä»¶
	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	// profiles ç”¨æ¥ä¿å­˜ä¸åŒè°ƒåº¦å™¨çš„ framework æ¡†æ¶, framework åˆ™ç”¨æ¥å­˜æ”¾ plugin.
	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		...
		...
	)

	// å®ä¾‹åŒ–å¿«ç…§
	snapshot := internalcache.NewEmptySnapshot()

	// å®ä¾‹åŒ– queue, è¯¥ queue ä¸º PriorityQueue.
	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		...
	)

	// å®ä¾‹åŒ– cache ç¼“å­˜
	schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything)

	// å®ä¾‹åŒ– scheduler å¯¹è±¡
	sched := &Scheduler{
		Cache:                    schedulerCache,
		client:                   client,
		nodeInfoSnapshot:         snapshot,
		NextPod:                  internalqueue.MakeNextPodFunc(podQueue),
		StopEverything:           stopEverything,
		SchedulingQueue:          podQueue,
	}
	sched.applyDefaultHandlers()

	// åœ¨ informer é‡Œæ³¨å†Œè‡ªå®šä¹‰çš„äº‹ä»¶å¤„ç†æ–¹æ³•
	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))

	return sched, nil
}

func (s *Scheduler) applyDefaultHandlers() {
	s.SchedulePod = s.schedulePod
	s.FailureHandler = s.handleSchedulingFailure
}
```

## æ³¨å†Œåœ¨ scheduler informer çš„å›è°ƒæ–¹æ³•

åœ¨ informer é‡Œæ³¨å†Œ pod å’Œ node èµ„æºçš„å›è°ƒæ–¹æ³•ï¼Œç›‘å¬ pod äº‹ä»¶å¯¹ queue å’Œ cache åšå›è°ƒå¤„ç†. ç›‘å¬ node äº‹ä»¶å¯¹ cache åšå¤„ç†.

```go
func addAllEventHandlers(
	sched *Scheduler,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	gvkMap map[framework.GVK]framework.ActionType,
) {
	// ç›‘å¬ pod äº‹ä»¶ï¼Œå¹¶æ³¨å†Œå¢åˆ æ”¹å›è°ƒæ–¹æ³•, å…¶æ“ä½œæ˜¯å¯¹ cache çš„å¢åˆ æ”¹
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				...
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)

	// ç›‘å¬ pod äº‹ä»¶ï¼Œå¹¶æ³¨å†Œå¢åˆ æ”¹æ–¹æ³•, å¯¹ queue æ’å…¥å¢åˆ æ”¹äº‹ä»¶.
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				...
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToSchedulingQueue,
				UpdateFunc: sched.updatePodInSchedulingQueue,
				DeleteFunc: sched.deletePodFromSchedulingQueue,
			},
		},
	)

	// ç›‘å¬ node äº‹ä»¶ï¼Œæ³¨å†Œå›è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ cache é‡Œå¯¹ node çš„å¢åˆ æ”¹æŸ¥.
	informerFactory.Core().V1().Nodes().Informer().AddEventHandler(
		cache.ResourceEventHandlerFuncs{
			AddFunc:    sched.addNodeToCache,
			UpdateFunc: sched.updateNodeInCache,
			DeleteFunc: sched.deleteNodeFromCache,
		},
	)
}
```

## scheduler çš„é€‰ä¸¾å®ç°

`kube-scheduler` è·Ÿ k8s çš„å…¶ä»–ä¸»æ§ç»„ä»¶ä¸€æ ·, ä¹Ÿä¼šé€šè¿‡é€‰ä¸¾ `leaderelection` æœºåˆ¶ä¿è¯é›†ç¾¤åªæœ‰ä¸€ä¸ª leader å®ä¾‹è¿è¡Œè°ƒåº¦å™¨, å…¶ä»– follower å®ä¾‹åˆ™å°è¯•è½®è¯¢æŠ¢é”ç›´åˆ°æˆåŠŸ.

æºç ä½ç½®: `cmd/kube-scheduler/app/server.go`

```go
func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	...

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := <-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}


	// å¯åŠ¨ informers, è¿™é‡Œåªæœ‰ pod å’Œ node.
	cc.InformerFactory.Start(ctx.Done())

	// åŒæ­¥ informer çš„æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
	cc.InformerFactory.WaitForCacheSync(ctx.Done())

	// å¦‚æœåœ¨é…ç½®ä¸­å¯åŠ¨äº†é€‰ä¸¾, åˆ›å»ºé€‰ä¸¾å¯¹è±¡, æ³¨å†Œäº‹ä»¶æ–¹æ³•, å¹¶å¯ç”¨é€‰ä¸¾.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				// å½“é€‰ä¸¾æ‹¿åˆ° leader æ—¶, å¯åŠ¨ scheduler è°ƒåº¦å™¨
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				// å½“é€‰ä¸¾æˆåŠŸä½†åé¢åˆä¸¢å¤± leader å, åˆ™é€€å‡ºè¿›ç¨‹.
				// è¿›ç¨‹é€€å‡ºå, ä¼šè¢« docker æˆ– systemd é‡æ–°æ‹‰èµ·, å°è¯•æ‹¿é”.
				select {
				case <-ctx.Done():
					// We were asked to terminate. Exit 0.
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, "Leaderelection lost")
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}

		// æ„å»º leaderelection å¯¹è±¡
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf("couldn't create leader elector: %v", err)
		}

		// å¯åŠ¨é€‰ä¸¾
		leaderElector.Run(ctx)

		return fmt.Errorf("lost lease")
	}

	// å¦‚æœæ²¡æœ‰å¼€å¯é€‰ä¸¾, åˆ™ç›´æ¥å¯åŠ¨ scheduler è°ƒåº¦å™¨.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf("finished without leader elect")
}
```

å…³äº client-go LeaderElection é€‰ä¸¾çš„å®ç°åŸç†, è¯·ç‚¹å‡»ä¸‹é¢è¿æ¥. 

[https://github.com/rfyiamcool/notes/blob/main/kubernetes_leader_election_code.md](https://github.com/rfyiamcool/notes/blob/main/kubernetes_leader_election_code.md)

## scheudler å¯åŠ¨å…¥å£

`Run()` æ–¹æ³•æ˜¯ k8s scheduler çš„å¯åŠ¨è¿è¡Œå…¥å£, å…¶æµç¨‹æ˜¯å…ˆå¯åŠ¨ queue é˜Ÿåˆ—çš„ Run æ–¹æ³•, å†å¼‚æ­¥å¯åŠ¨ä¸€ä¸ªåç¨‹å¤„ç†æ ¸å¿ƒè°ƒåº¦æ–¹æ³• `scheduleOne`.

`schedulingQueue` çš„ `Run()` æ–¹æ³•ç”¨æ¥ç›‘å¬å†…éƒ¨çš„å»¶è¿Ÿä»»åŠ¡, æŠŠåˆ°æœŸçš„ä»»åŠ¡æ”¾åˆ° activeQ ä¸­.

è€Œ `scheduleOne` æ–¹æ³•ç”¨æ¥ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œè·å–ç”± informer æ’å…¥çš„ pod å¯¹è±¡, è°ƒç”¨ `schedulingCycle` ä¸º pod é€‰æ‹©æœ€ä¼˜çš„ node èŠ‚ç‚¹. å¦‚æœæ‰¾åˆ°äº†åˆé€‚çš„ node èŠ‚ç‚¹, åˆ™è°ƒç”¨ `bindingCycle` æ–¹æ³•æ¥å‘èµ· pod å’Œ node ç»‘å®š.

æºç ä½ç½®: `pkg/scheduler/scheduler.go`

```go
func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	<-ctx.Done()
	sched.SchedulingQueue.Close()
}

func (sched *Scheduler) scheduleOne(ctx context.Context) {
	// ä» activeQ ä¸­è·å–éœ€è¦è°ƒåº¦çš„ pod æ•°æ®
	podInfo := sched.NextPod()
	pod := podInfo.Pod

	// ä¸º pod é€‰æ‹©æœ€ä¼˜çš„ node èŠ‚ç‚¹
	scheduleResult, assumedPodInfo, err := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate)
	if err != nil {
		// å¦‚ä½•æ²¡æœ‰æ‰¾åˆ°èŠ‚ç‚¹ï¼Œåˆ™æ‰§è¡Œå¤±è´¥æ–¹æ³•.
		sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, err, scheduleResult.reason, scheduleResult.nominatingInfo, start)
		return
	}

	go func() {
		// åƒ apiserver å‘èµ· pod -> node ç»‘å®š
		status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate)
		if !status.IsSuccess() {
			sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status)
		}
	}()
}
```

`NextPod` åº•å±‚å¼•ç”¨äº† `MakeNextPodFunc` æ–¹æ³•, å…¶å†…éƒ¨ä» `PriorityQueue` é˜Ÿåˆ—ä¸­è·å– pod å¯¹è±¡.

```go
func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			return podInfo
		}
		return nil
	}
}

func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
	p.lock.Lock()
	defer p.lock.Unlock()

	for p.activeQ.Len() == 0 {
		// æ²¡æœ‰æ•°æ®åˆ™é™·å…¥æ¡ä»¶å˜é‡çš„ç­‰å¾…æ¥å£
		p.cond.Wait()
	}
	obj, err := p.activeQ.Pop()
	if err != nil {
		return nil, err
	}
	pInfo := obj.(*framework.QueuedPodInfo)
	pInfo.Attempts++  // æ¯æ¬¡ pop åéƒ½å¢åŠ ä¸‹ attempts æ¬¡æ•°.
	return pInfo, nil
}
```

## å•å®ä¾‹å•åç¨‹æ¨¡å‹çš„ schduler è°ƒåº¦å™¨

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301261858892.png)

éœ€è¦å…³æ³¨çš„æ˜¯æ•´ä¸ª kubernetes scheduler è°ƒåº¦å™¨åªæœ‰ä¸€ä¸ªåç¨‹å¤„ç†ä¸»è°ƒåº¦å¾ªç¯ `scheduleOne`, è™½ç„¶ kubernetes scheduler å¯ä»¥å¯åŠ¨å¤šä¸ªå®ä¾‹, ä½†å¯åŠ¨æ—¶éœ€è¦ leaderelection é€‰ä¸¾, åªæœ‰ leader æ‰å¯ä»¥å¤„ç†è°ƒåº¦, å…¶ä»–èŠ‚ç‚¹ä½œä¸º follower ç­‰å¾… leader å¤±æ•ˆ. ä¹Ÿå°±æ˜¯è¯´æ•´ä¸ª k8s é›†ç¾¤è°ƒåº¦æ ¸å¿ƒçš„å¹¶å‘åº¦ä¸º 1 ä¸ª. 

äº‘åŸç”Ÿç¤¾åŒºä¸­æœ‰äººä½¿ç”¨ kubemark æ¨¡æ‹Ÿ 2000 ä¸ªèŠ‚ç‚¹çš„è§„æ¨¡æ¥å‹æµ‹ kube-scheduler å¤„ç†æ€§èƒ½åŠæ—¶å»¶, æµ‹è¯•ç»“æœæ˜¯ 30s å†…å®Œæˆ 15000 ä¸ª pod è°ƒåº¦ä»»åŠ¡. è™½ç„¶ kube-scheduler æ˜¯å•å¹¶å‘æ¨¡å‹, ä½†ç”±äºé¢„é€‰å’Œä¼˜é€‰éƒ½å±äºè®¡ç®—å‹ä»»åŠ¡éé˜»å¡IO, åˆæœ‰ `percentageOfNodesToScore` å‚æ•°ä¼˜åŒ–, æœ€é‡è¦çš„æ˜¯åˆ›å»º pod çš„æ“ä½œé€šå¸¸ä¸ä¼šå¤ªé«˜å¹¶å‘. è¿™å‡ ç‚¹ä¸‹æ¥å•å¹¶å‘æ¨¡å‹çš„ scheduler ä¹Ÿè¿˜å¯ä»¥æ¥å—çš„.

### ä¸ºä»€ä¹ˆ scheduler ä¸æ”¯æŒå¹¶å‘ ?

æŒ‰ç…§å½“å‰ scheudler è°ƒåº¦å™¨çš„è®¾è®¡åŸç†, ä½¿ç”¨é¢„é€‰å’Œä¼˜é€‰ç®—æ³•é€‰å‡ºæœ€åˆé€‚çš„èŠ‚ç‚¹, å¹¶å‘åœºæ™¯ä¸‹æ— æ³•ä¿è¯å®‰å…¨, æ¯”å¦‚, é€‰å‡ºçš„æœ€ä¼˜èŠ‚ç‚¹åœ¨å¹¶å‘ä¸‹ä¼šè¢«å¤šä¸ª pod ç»‘å®š.

### ä½¿ç”¨è‡ªå®šä¹‰è°ƒåº¦å™¨è¿›è¡Œå¹¶å‘è°ƒåº¦ ?

k8s é»˜è®¤çš„è°ƒåº¦å™¨ä¸º `default-scheduler`, è€Œä½¿ç”¨ç›¸åŒè°ƒåº¦å™¨åªèƒ½å•å¹¶å‘å¤„ç†è°ƒåº¦. ä½†æ˜¯å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰å®ç°è°ƒåº¦å™¨çš„æ–¹æ¡ˆ, åœ¨åˆ›å»º pod æ—¶æŒ‡å®šä¸åŒçš„è°ƒåº¦å™¨ç®—æ³• `pod.Spec.schedulerName = xiaorui.cc`, è¿™æ ·å¯ä»¥ä½¿ä¸åŒè°ƒåº¦å™¨çš„ kube-schedueler å¹¶è¡Œè°ƒåº¦èµ·æ¥, å„è‡ªæŒ‰ç…§è°ƒåº¦ç®—æ³•æ¥è°ƒåº¦, è¿è¡Œäº’ä¸å½±å“.

å½“ç„¶å¤§å¤šæ•°å…¬å¸æ²¡è¿™ä¸ªå¿…è¦.

## schedulingCycle æ ¸å¿ƒè°ƒåº¦å‘¨æœŸçš„å®ç°

`schedulingCycle()` è¯¥æ–¹æ³•ä¸»è¦ä¸º pod é€‰å‡ºæœ€ä¼˜çš„ node èŠ‚ç‚¹. å…ˆé€šè¿‡é¢„é€‰è¿‡ç¨‹è¿‡æ»¤å‡ºç¬¦åˆ pod è¦æ±‚çš„èŠ‚ç‚¹é›†åˆ, å†é€šè¿‡æ’ä»¶å¯¹è¿™äº›èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†, ä½¿ç”¨åˆ†å€¼æœ€é«˜çš„ node ä¸º pod è°ƒåº¦èŠ‚ç‚¹.

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301251629071.png)

scheduler å†…ç½®å„ä¸ªé˜¶æ®µçš„å„ç§æ’ä»¶, é¢„é€‰å’Œä¼˜é€‰é˜¶æ®µå°±æ˜¯éå†å›è°ƒæ’ä»¶æ±‚å‡ºç»“æœ.

è°ƒåº¦å‘¨æœŸ `schedulingCycle` å†…å…³é”®æ–¹æ³•æ˜¯ `schedulePod`, å…¶ç®€åŒ–æµç¨‹å¦‚ä¸‹.

1. å…ˆè°ƒç”¨ `findNodesThatFitPod` è¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„é¢„é€‰èŠ‚ç‚¹.
2. è°ƒç”¨ `prioritizeNodes` ä¸ºé¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ† score.
3. æœ€åè°ƒç”¨ `selectHost` é€‰æ‹©æœ€åˆé€‚çš„ node èŠ‚ç‚¹.

```go
func (sched *Scheduler) schedulingCycle(
	...
) (ScheduleResult, *framework.QueuedPodInfo, error) {

	pod := podInfo.Pod

	// é€‰æ‹©èŠ‚ç‚¹
	scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod)
	...

	// åœ¨ç¼“å­˜ cache ä¸­æ›´æ–°çŠ¶æ€
	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
	...
}

func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	// æ›´æ–°å¿«ç…§
	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}

	// è¿›è¡Œé¢„é€‰ç­›é€‰
	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}

	// é¢„é€‰ä¸‹æ¥ï¼Œæ— èŠ‚ç‚¹å¯ä»¥ç”¨, è¿”å›é”™è¯¯
	if len(feasibleNodes) == 0 {
		return result, &framework.FitError{
			Pod:         pod,
			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

	// ç»è¿‡é¢„é€‰å°±åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé‚£ä¹ˆç›´æ¥è¿”å›
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost:  feasibleNodes[0].Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes:  1,
		}, nil
	}

	// è¿›è¡Œä¼˜é€‰è¿‡ç¨‹, å¯¹é¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†
	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	// ä»ä¼˜é€‰ä¸­é€‰å‡ºæœ€é«˜åˆ†çš„èŠ‚ç‚¹
	host, err := selectHost(priorityList)

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes:  len(feasibleNodes),
	}, err
}
```

### findNodesThatFitPod

`findNodesThatFitPod` æ–¹æ³•ç”¨æ¥å®ç°è°ƒåº¦å™¨çš„é¢„é€‰è¿‡ç¨‹, å…¶å†…éƒ¨è°ƒç”¨æ’ä»¶çš„ PreFilter å’Œ Filter æ–¹æ³•æ¥ç­›é€‰å‡ºç¬¦åˆ pod è¦æ±‚çš„ node èŠ‚ç‚¹é›†åˆ.

```go
func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	diagnosis := framework.Diagnosis{
		NodeToStatusMap:      make(framework.NodeToStatusMap),
		UnschedulablePlugins: sets.NewString(),
	}

	// è·å–æ‰€æœ‰çš„ nodes
	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}

	// è°ƒç”¨ framework çš„ PreFilter é›†åˆé‡Œçš„æ’ä»¶
	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
	if !s.IsSuccess() {
		// å¦‚æœåœ¨ prefilter æœ‰å¼‚å¸¸, åˆ™ç›´æ¥è·³å‡º.
		if !s.IsUnschedulable() {
			return nil, diagnosis, s.AsError()
		}
		msg := s.Message()
		diagnosis.PreFilterMsg = msg
		return nil, diagnosis, nil
	}

	...

	// æ ¹æ® prefilter æ‹¿åˆ°çš„ node names è·å– node info å¯¹è±¡.
	nodes := allNodes
	if !preRes.AllNodes() {
		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
		for n := range preRes.NodeNames {
			nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)
			if err != nil {
				return nil, diagnosis, err
			}
			nodes = append(nodes, nInfo)
		}
	}

	// è¿è¡Œ framework çš„ filter æ’ä»¶åˆ¤æ–­ node æ˜¯å¦å¯ä»¥è¿è¡Œæ–° pod.
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)

	...

	// è°ƒç”¨é¢å¤–çš„ extender è°ƒåº¦å™¨æ¥è¿›è¡Œé¢„é€‰
	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
```

#### findNodesThatPassFilters å¹¶å‘æ‰§è¡Œ Filter æ’ä»¶æ–¹æ³•

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301252324752.png)

`findNodesThatPassFilters` æ–¹æ³•ç”¨æ¥éå†æ‰§è¡Œ framework é‡Œ Filter æ’ä»¶é›†åˆçš„ Filter æ–¹æ³•.

ä¸ºäº†åŠ å¿«æ‰§è¡Œæ•ˆç‡, å‡å°‘é¢„é€‰é˜¶æ®µçš„æ—¶å»¶, framework å†…éƒ¨æœ‰ä¸ª Parallelizer å¹¶å‘æ§åˆ¶å™¨, å¯ç”¨ 16 ä¸ªåç¨‹å¹¶å‘è°ƒç”¨æ’ä»¶çš„ Filter æ–¹æ³•. åœ¨å¤§é›†ç¾¤ä¸‹ nodes èŠ‚ç‚¹ä¼šå¾ˆå¤š, ä¸ºäº†é¿å…éå†å…¨é‡çš„ nodes æ‰§è¡Œ Filter å’Œåç»­çš„æ’ä»¶é€»è¾‘, è¿™é‡Œé€šè¿‡ `numFeasibleNodesToFind` æ–¹æ³•æ¥å‡å°‘æ‰«æè®¡ç®—çš„ nodes æ•°é‡.

å½“æˆåŠŸæ‰§è¡Œ filter æ’ä»¶æ–¹æ³•çš„æ•°é‡è¶…è¿‡ numNodesToFind æ—¶, åˆ™æ‰§è¡Œ context cancel(). è¿™æ · framework å¹¶å‘åç¨‹æ± ç›‘å¬åˆ° ctx è¢«å…³é—­å, åˆ™ä¸ä¼šç»§ç»­æ‰§è¡Œåç»­çš„ä»»åŠ¡.

```go
func (sched *Scheduler) findNodesThatPassFilters(
	ctx context.Context,
	fwk framework.Framework,
	pod *v1.Pod,
	nodes []*framework.NodeInfo) ([]*v1.Node, error) {

	numAllNodes := len(nodes)
	// è®¡ç®—éœ€è¦æ‰«æçš„ nodes æ•°, é¿å…äº†è¶…å¤§é›†ç¾¤ä¸‹ nodes çš„è®¡ç®—æ•°é‡.
	// å½“é›†ç¾¤çš„èŠ‚ç‚¹æ•°å°äº 100 æ—¶, åˆ™ç›´æ¥ä½¿ç”¨é›†ç¾¤çš„èŠ‚ç‚¹æ•°ä½œä¸ºæ‰«ææ•°æ®é‡
	// å½“å¤§äº 100 æ—¶, åˆ™ä½¿ç”¨å…¬å¼è®¡ç®— `numAllNodes * (50 - numAllNodes/125) / 100`
	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))

	feasibleNodes := make([]*v1.Node, numNodesToFind)

	// å¦‚æœ framework æœªæ³¨å†Œ Filter æ’ä»¶, åˆ™é€€å‡º.
	if !fwk.HasFilterPlugins() {
		for i := range feasibleNodes {
			feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes].Node()
		}
		return feasibleNodes, nil
	}

	// framework å†…ç½®å¹¶å‘æ§åˆ¶å™¨, å¹¶å‘ 16 ä¸ªåç¨‹å»è¯·æ±‚æ’ä»¶çš„ Filter æ–¹æ³•.
	errCh := parallelize.NewErrorChannel()
	var statusesLock sync.Mutex

	checkNode := func(i int) {
		// è·å– node info å¯¹è±¡
		nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]

		// éå†æ‰§è¡Œ framework çš„ Filter æ’ä»¶çš„ Filter æ–¹æ³•.
		status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
		if status.Code() == framework.Error {
			// å¦‚æœæœ‰é”™è¯¯, ç›´æ¥æŠŠé”™è¯¯ä¼ åˆ° errCh ç®¡é“é‡Œ.
			errCh.SendErrorWithCancel(status.AsError(), cancel)
			return
		}
		if status.IsSuccess() {
			// å¦‚æœæˆåŠŸæ‰§è¡Œ Filter æ’ä»¶çš„æ•°é‡è¶…è¿‡ numNodesToFind, åˆ™æ‰§è¡Œ cancel().
			// å½“ ctx è¢« cancel(), framework çš„å¹¶å‘åç¨‹æ± ä¸ä¼šç»§ç»­æ‰§è¡Œåç»­çš„ä»»åŠ¡.
			length := atomic.AddInt32(&feasibleNodesLen, 1)
			if length > numNodesToFind {
				cancel()
				atomic.AddInt32(&feasibleNodesLen, -1)
			} else {
				feasibleNodes[length-1] = nodeInfo.Node()
			}
		}
		...
	}

	// å¹¶å‘è°ƒç”¨ framework çš„ Filter æ’ä»¶çš„ Filter æ–¹æ³•.
	fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, frameworkruntime.Filter)
	feasibleNodes = feasibleNodes[:feasibleNodesLen]
	if err := errCh.ReceiveError(); err != nil {
		// å½“æœ‰é”™è¯¯æ—¶, ç›´æ¥è¿”å›.
		statusCode = framework.Error
		return feasibleNodes, err
	}

	// è¿”å›å¯ç”¨çš„ nodes åˆ—è¡¨.
	return feasibleNodes, nil
}
```

framework çš„å¹¶å‘åº“é€šè¿‡å°è£… `workqueue.ParallelizeUntil` æ¥å®ç°.

`k8s.io/client-go/util/workqueue/parallelizer.go`

#### numFeasibleNodesToFind è®¡ç®—å¤šå°‘èŠ‚ç‚¹å‚ä¸é¢„é€‰

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301261120469.png)

ğŸ¤” è€ƒè™‘ä¸€ä¸ªé—®é¢˜, å½“ k8s çš„ node èŠ‚ç‚¹ç‰¹åˆ«å¤šæ—¶, è¿™äº›èŠ‚ç‚¹éƒ½è¦å‚ä¸é¢„å…ˆçš„è°ƒåº¦è¿‡ç¨‹ä¹ˆ ? æ¯”å¦‚å¤§é›†ç¾¤æœ‰ 2500 ä¸ªèŠ‚ç‚¹, æ³¨å†Œçš„æ’ä»¶æœ‰ 10 ä¸ª, é‚£ä¹ˆ ç­›é€‰ Filter å’Œ æ‰“åˆ† Score è¿‡ç¨‹éœ€è¦è¿›è¡Œ 2500 * 10 * 2 = 50000 æ¬¡è®¡ç®—, æœ€åé€‰å®šä¸€ä¸ªæœ€é«˜åˆ†å€¼çš„èŠ‚ç‚¹æ¥ç»‘å®š pod. k8s scheduler è€ƒè™‘åˆ°äº†è¿™æ ·çš„æ€§èƒ½å¼€é”€, æ‰€ä»¥åŠ å…¥äº†ç™¾åˆ†æ¯”å‚æ•°æ§åˆ¶å‚ä¸é¢„é€‰çš„èŠ‚ç‚¹æ•°.

`numFeasibleNodesToFind` æ–¹æ³•æ ¹æ®å½“å‰é›†ç¾¤çš„èŠ‚ç‚¹æ•°è®¡ç®—å‡ºå‚ä¸é¢„é€‰çš„èŠ‚ç‚¹æ•°é‡, æŠŠå‚ä¸ Filter çš„èŠ‚ç‚¹èŒƒå›´ç¼©å°, æ— éœ€å…¨é¢æ‰«ææ‰€æœ‰çš„èŠ‚ç‚¹, è¿™æ ·é¿å… k8s é›†ç¾¤ nodes å¤ªå¤šæ—¶, é€ æˆæ— æ•ˆçš„è®¡ç®—èµ„æºå¼€é”€.

`numFeasibleNodesToFind` ç­–ç•¥æ˜¯è¿™æ ·çš„, å½“é›†ç¾¤èŠ‚ç‚¹å°äº 100 æ—¶, é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½å‚ä¸é¢„é€‰. è€Œå½“å¤§äº 100 æ—¶, åˆ™ä½¿ç”¨ä¸‹é¢çš„å…¬å¼è®¡ç®—æ‰«ææ•°. scheudler çš„ `percentageOfNodesToScore` å‚æ•°é»˜è®¤ä¸º 0, æºç ä¸­ä¼šèµ‹å€¼ä¸º 50 %.

```
numAllNodes * (50 - numAllNodes/125) / 100
```

å‡è®¾å½“å‰é›†ç¾¤æœ‰ `500` ä¸ª nodes èŠ‚ç‚¹, é‚£ä¹ˆéœ€è¦æ‰§è¡Œ Filter æ’ä»¶æ–¹æ³•çš„ nodee èŠ‚ç‚¹æœ‰ `500 * (50 - 500/125) / 100 = 230` ä¸ª.

`numFeasibleNodesToFind` åªæ˜¯è¡¨æ˜æ‰«åˆ°è¿™ä¸ªèŠ‚ç‚¹æ•°åå°±ç»“æŸäº†, ä½†å¦‚æœå‰é¢æ‰§è¡Œæ’ä»¶å‘ç”Ÿå¤±è´¥æ—¶, è‡ªç„¶ä¼šåŠ å¤§æ‰«ææ•°.

```go
func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
	// å½“å‰çš„é›†ç¾¤å°äº 100 æ—¶, åˆ™ç›´æ¥ä½¿ç”¨é›†ç¾¤èŠ‚ç‚¹æ•°ä½œä¸ºæ‰«ææ•°
	if numAllNodes < minFeasibleNodesToFind {
		return numAllNodes
	}

	// k8s scheduler çš„ nodes ç™¾åˆ†æ¯”é»˜è®¤ä¸º 0
	var percentage int32
	if percentageOfNodesToScore != nil {
		percentage = *percentageOfNodesToScore
	} else {
		percentage = sched.percentageOfNodesToScore
	}

	if percentage == 0 {
		percentage = int32(50) - numAllNodes/125
		// ä¸èƒ½å°äº 5
		if percentage < minFeasibleNodesPercentageToFind {
			percentage = minFeasibleNodesPercentageToFind
		}
	}

	// éœ€è¦æ‰«æçš„èŠ‚ç‚¹æ•°
	numNodes = numAllNodes * percentage / 100
	// å¦‚æœå°äº 100, åˆ™ä½¿ç”¨ 100 ä½œä¸ºæ‰«ææ•°. 
	if numNodes < minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}
```

### prioritizeNodes

`prioritizeNodes` æ–¹æ³•ä¸ºè°ƒåº¦å™¨çš„ä¼˜é€‰é˜¶æ®µçš„å®ç°. å…¶å†…éƒ¨ä¼šéå†è°ƒç”¨ framework çš„ PreScore æ’ä»¶é›†åˆé‡Œ `PeScore` æ–¹æ³•, ç„¶åå†éå†è°ƒç”¨ framework çš„ Score æ’ä»¶é›†åˆçš„ `Score` æ–¹æ³•. ç»è¿‡ Score æ‰“åˆ†è®¡ç®—åå¯ä»¥æ‹¿åˆ°å„ä¸ª node çš„åˆ†å€¼.

```go
func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*v1.Node,
) ([]framework.NodePluginScores, error) {
	// å¦‚æœ extenders ä¸ºç©ºå’Œscore æ’ä»¶ä¸ºç©º, åˆ™è·³å‡º
	if len(extenders) == 0 && !fwk.HasScorePlugins() {
		result := make([]framework.NodePluginScores, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodePluginScores{
				Name:       nodes[i].Name,
				TotalScore: 1,
			})
		}
		return result, nil
	}

	// åœ¨ framework çš„ PreScore æ’ä»¶é›†åˆé‡Œ, éå†æ‰§è¡Œæ’ä»¶çš„ PreSocre æ–¹æ³•
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		// åªæœ‰æœ‰å¼‚å¸¸ç›´æ¥é€€å‡º
		return nil, preScoreStatus.AsError()
	}

	// åœ¨ framework çš„ Score æ’ä»¶é›†åˆé‡Œ, éå†æ‰§è¡Œæ’ä»¶çš„ Socre æ–¹æ³•
	nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	klogV := klog.V(10)
	if klogV.Enabled() {
		for _, nodeScore := range nodesScores {
			// æ‰“å°æ’ä»¶åå­—å’Œåˆ†å€¼ score
			for _, pluginScore := range nodeScore.Scores {
				klogV.InfoS("Plugin scored node for pod", "pod", klog.KObj(pod), "plugin", pluginScore.Name, "node", nodeScore.Name, "score", pluginScore.Score)
			}
		}
	}

	if len(extenders) != 0 && nodes != nil {
		// å½“é¢å¤– extenders è°ƒåº¦å™¨ä¸ä¸ºç©ºæ—¶, åˆ™éœ€è¦è®¡ç®—åˆ†å€¼.
		...
		...
	}

	return nodesScores, nil
}
```

### selectHost

`selectHost` æ˜¯ä»ä¼˜é€‰çš„ nodes é›†åˆé‡Œè·å–åˆ†å€¼ socre æœ€é«˜çš„ node. å†…éƒ¨è¿˜åšäº†ä¸€ä¸ªå°ä¼˜åŒ–, å½“ç›¸è¿‘çš„ä¸¤ä¸ª node åˆ†å€¼ç›¸åŒæ—¶, åˆ™é€šè¿‡éšæœºæ¥é€‰æ‹© node, ç›®çš„ä½¿ k8s node çš„è´Ÿè½½æ›´è¶‹äºå‡è¡¡.

```go
func selectHost(nodeScores []framework.NodePluginScores) (string, error) {
	// å¦‚æœ nodes ä¸ºç©º, åˆ™è¿”å›é”™è¯¯
	if len(nodeScores) == 0 {
		return "", fmt.Errorf("empty priorityList")
	}

	// ç›´æ¥ä»å¤´åˆ°ä½éå† nodeScores æ•°ç»„, æ‹¿åˆ°åˆ†å€¼ score æœ€åçš„ nodeName.
	maxScore := nodeScores[0].TotalScore
	selected := nodeScores[0].Name
	cntOfMaxScore := 1
	for _, ns := range nodeScores[1:] {
		if ns.TotalScore > maxScore {
			// å½“å‰çš„åˆ†å€¼æ›´å¤§, åˆ™è¿›è¡Œèµ‹å€¼.
			maxScore = ns.TotalScore
			selected = ns.Name
			cntOfMaxScore = 1
		} else if ns.TotalScore == maxScore {
			// å½“ä¸¤ä¸ª node çš„ åˆ†å€¼ç›¸åŒæ—¶, 
			// ä½¿ç”¨éšæœºç®—æ³•æ¥é€‰æ‹©å½“å‰å’Œä¸Šä¸€ä¸ª node.
			cntOfMaxScore++
			if rand.Intn(cntOfMaxScore) == 0 {
				selected = ns.Name
			}
		}
	}

	// è¿”å›åˆ†å€¼æœ€é«˜çš„ node
	return selected, nil
}
```

### PriorityQueue çš„å®ç°

`PriorityQueue` ç”¨æ¥å®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—, informer ä¼šæ¡ä»¶ pod åˆ° priorityQueue é˜Ÿåˆ—ä¸­, `scheduleOne` ä¼šä»è¯¥é˜Ÿåˆ—ä¸­ pop å¯¹è±¡. åœ¨åˆ›å»ºå»¶è¿Ÿé˜Ÿåˆ—æ—¶ä¼ å…¥ä¸€ä¸ª `less` æ¯”è¾ƒæ–¹æ³•, æ—¶é—´æœ€å°çš„ podInfo æ”¾åœ¨ heap çš„æœ€é¡¶ç«¯.

`flushBackoffQCompleted` ä¼šä¸æ–­çš„æ£€æŸ¥ backoff heap å †é¡¶çš„å…ƒç´ æ˜¯å¦æ»¡è¶³æ¡ä»¶, å½“æ»¡è¶³æ¡ä»¶æŠŠ pod å¯¹è±¡æ‰”åˆ° activeQ é˜Ÿé‡Œ, å¹¶æ¿€æ´»æ¡ä»¶å˜é‡. è¿™é‡Œæ²¡æœ‰é‡‡ç”¨ç›‘å¬ç­‰å¾…å †é¡¶åˆ°æœŸæ—¶é—´çš„æ–¹æ³•ï¼Œè€Œæ˜¯æ¯éš”ä¸€ç§’å»æ£€æŸ¥å †é¡¶çš„ podInfo æ˜¯å¦å·²åˆ°æœŸ `isPodBackingoff`.

backoff æ—¶é•¿æ˜¯ä¾èµ– podInfo.Attempts é‡è¯•æ¬¡æ•°çš„ï¼Œé»˜è®¤æƒ…å†µä¸‹é‡è¯• 5æ¬¡ æ˜¯ 5s, æœ€å¤§ä¸èƒ½è¶…è¿‡ 10s. ä¸»è°ƒåº¦æ–¹æ³• `scheduleOne` æ¯æ¬¡ä»é˜Ÿåˆ—è·å– podInfo æ—¶, å®ƒçš„ Attempts å­—æ®µéƒ½ä¼šåŠ ä¸€.

ä»£ç ä½ç½®: `pkg/scheduler/internal/queue/scheduling_queue.go`

```go
const (
	DefaultPodMaxBackoffDuration time.Duration = 10 * time.Second
)

// å®ä¾‹åŒ–ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œè¯¥é˜Ÿåˆ—ä¸­å«æœ‰ activeQ, podBackoffQ, unschedulablePodsé›†åˆ.
func NewPriorityQueue() {
	pq.podBackoffQ = heap.NewWithRecorder(
		podInfoKeyFunc, 
		pq.podsCompareBackoffCompleted, 
		...
	)
}

// æ·»åŠ  pod å¯¹è±¡
func (p *PriorityQueue) Add(pod *v1.Pod) error {
	p.lock.Lock()
	defer p.lock.Unlock()

	pInfo := p.newQueuedPodInfo(pod)
	gated := pInfo.Gated

	// æŠŠå¯¹è±¡åŠ åˆ° activeQ é˜Ÿåˆ—é‡Œ.
	if added, err := p.addToActiveQ(pInfo); !added {
		return err
	}
	// å¦‚æœè¯¥å¯¹è±¡åœ¨ä¸å¯è°ƒåº¦é›†åˆä¸­å­˜åœ¨, åˆ™éœ€è¦åœ¨é‡Œé¢åˆ é™¤.
	if p.unschedulablePods.get(pod) != nil {
		p.unschedulablePods.delete(pod, gated)
	}

	// ä» backoffQ åˆ é™¤ pod å¯¹è±¡
	if err := p.podBackoffQ.Delete(pInfo); err == nil {
		klog.ErrorS(nil, "Error: pod is already in the podBackoff queue", "pod", klog.KObj(pod))
	}

	// æ¡ä»¶å˜é‡é€šçŸ¥
	p.cond.Broadcast()

	return nil
}

// è·å– pod info å¯¹è±¡
func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
	p.lock.Lock()
	defer p.lock.Unlock()

	// å¦‚æœ activeQ ä¸ºç©º, é™·å…¥ç­‰å¾…
	for p.activeQ.Len() == 0 {
		if p.closed {
			return nil, fmt.Errorf(queueClosed)
		}
		p.cond.Wait()
	}

	// ä» activeQ å †é¡¶ pop å¯¹è±¡
	obj, err := p.activeQ.Pop()
	if err != nil {
		return nil, err
	}

	pInfo := obj.(*framework.QueuedPodInfo)
	// åŠ ä¸€
	pInfo.Attempts++
	p.schedulingCycle++
	return pInfo, nil
}

// heap çš„æ¯”è¾ƒæ–¹æ³•ï¼Œç¡®ä¿ deadline æœ€ä½çš„åœ¨ heap é¡¶éƒ¨.
func (p *PriorityQueue) podsCompareBackoffCompleted(podInfo1, podInfo2 interface{}) bool {
	bo1 := p.getBackoffTime(pInfo1)
	bo2 := p.getBackoffTime(pInfo2)
	return bo1.Before(bo2)
}

func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time {
	duration := p.calculateBackoffDuration(podInfo)
	backoffTime := podInfo.Timestamp.Add(duration)
	return backoffTime
}

// backoff duration éšç€é‡è¯•æ¬¡æ•°ä¸æ–­å åŠ ï¼Œä½†æœ€å¤§ä¸èƒ½è¶…è¿‡ maxBackoffDuration.
func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
	duration := p.podInitialBackoffDuration // 1s
	for i := 1; i < podInfo.Attempts; i++ {
		if duration > p.podMaxBackoffDuration-duration {
			return p.podMaxBackoffDuration // 10s
		}
		duration += duration
	}
	return duration
}
```

### å¦‚ä½•å¤„ç†è°ƒåº¦å¤±è´¥çš„ pod

å‰é¢æœ‰è¯´ kubernetes scheduler çš„ `scheduleOne` ä½œä¸ºä¸»å¾ªç¯å¤„ç†å‡½æ•°ï¼Œå½“æ²¡æœ‰ä¸º pod æ‰¾åˆ°åˆé€‚ node æ—¶ï¼Œä¼šè°ƒç”¨ `FailureHandler` æ–¹æ³•.

`FailureHandler()` æ˜¯ç”± `handleSchedulingFailure()` æ–¹æ³•å®ç°. è¯¥é€»è¾‘çš„å®ç°ç®€å•è¯´å°±æ˜¯æŠŠå¤±è´¥çš„ pod æ‰”åˆ° podBackoffQ é˜Ÿåˆ—æˆ–è€… unschedulablePods é›†åˆé‡Œ.

```go
func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, err error, reason string, nominatingInfo *framework.NominatingInfo, start time.Time) {
	podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister()
	cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name)
	if e != nil {
	} else {
		if len(cachedPod.Spec.NodeName) != 0 {
			klog.InfoS("Pod has been assigned to node. Abort adding it back to queue.", "pod", klog.KObj(pod), "node", cachedPod.Spec.NodeName)
		} else {
			podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy())

			// é‡æ–°å…¥é˜Ÿåˆ—
			if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil {
				klog.ErrorS(err, "Error occurred")
			}
		}
	}

	...
}

func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	pod := pInfo.Pod

	// å»é‡åˆ¤æ–­
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf("Pod %v is already present in the active queue", klog.KObj(pod))
	}
	// å»é‡åˆ¤æ–­
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf("Pod %v is already present in the backoff queue", klog.KObj(pod))
	}

	// æŠŠæ²¡æœ‰è°ƒåº¦æˆåŠŸçš„ podInfo æ‰”åˆ° backoffQ é˜Ÿåˆ—æˆ–è€… unschedulablePods é›†åˆä¸­.
	if p.moveRequestCycle >= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf("error adding pod %v to the backoff queue: %v", klog.KObj(pod), err)
		}
	} else {
		p.unschedulablePods.addOrUpdate(pInfo)
	}

	return nil
}
```

scheduler queue åœ¨å¯åŠ¨æ—¶ä¼šå¼€å¯ä¸¤ä¸ªå¸¸é©»çš„åç¨‹. ä¸€ä¸ªåç¨‹æ¥ç®¡ç† `flushBackoffQCompleted()`ï¼Œæ¯éš”ä¸€ç§’æ¥è°ƒç”¨ä¸€æ¬¡. å¦ä¸€ä¸ªåç¨‹æ¥ç®¡ç† `flushUnschedulablePodsLeftover`, æ¯éš”ä¸‰åç§’æ¥è°ƒç”¨ä¸€æ¬¡.

```go
// Run starts the goroutine to pump from podBackoffQ to activeQ
func (p *PriorityQueue) Run() {
	go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop)
	go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)
}
```

`flushBackoffQCompleted` ä» podBackoffQ è·å– podInfo, ç„¶åæ‰”åˆ° activeQ é‡Œï¼Œç­‰å¾… scheduleOne æ¥è°ƒåº¦å¤„ç†.

```go
func (p *PriorityQueue) flushBackoffQCompleted() {
	activated := false
	for {
		rawPodInfo := p.podBackoffQ.Peek()
		pInfo := rawPodInfo.(*framework.QueuedPodInfo)
		pod := pInfo.Pod
        
		// ä» podBackoffQ ä¸­è·å–ä¸Šæ¬¡è°ƒåº¦å¤±è´¥çš„ podInfo.
		_, err := p.podBackoffQ.Pop()
		if err != nil {
			klog.ErrorS(err, "Unable to pop pod from backoff queue despite backoff completion", "pod", klog.KObj(pod))
			break
		}
        
		// ç„¶åæŠŠè¿™ podInfo å†æ‰”åˆ° activeQ é‡Œï¼Œè®© scheduleOne ä¸»å¾ªç¯æ¥å¤„ç†.
		if added, _ := p.addToActiveQ(pInfo); added {
			klog.V(5).InfoS("Pod moved to an internal scheduling queue", "pod", klog.KObj(pod), "event", BackoffComplete, "queue", activeQName)
			activated = true
		}
	}

	// å¦‚æœæœ‰æ·»åŠ æˆåŠŸçš„ï¼Œåˆ™æ¿€æ´»æ¡ä»¶å˜é‡.
	if activated {
		p.cond.Broadcast()
	}
}
```

`flushUnschedulablePodsLeftover` åŠ é”éå† PodInfoMap, å¦‚æœæŸä¸ª pod çš„è·ç¦»ä¸Šæ¬¡çš„è°ƒåº¦æ—¶é—´å¤§äº60s, åˆ™æ‰”åˆ°ä¸¤ä¸ªé˜Ÿåˆ—ä¸­çš„ä¸€ä¸ªï¼Œå¦åˆ™ç­‰å¾…ä¸‹ä¸ª 30s å†æ¥å¤„ç†.

```go
func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
	p.lock.Lock()
	defer p.lock.Unlock()

	var podsToMove []*framework.QueuedPodInfo
	for _, pInfo := range p.unschedulablePods.podInfoMap {
		lastScheduleTime := pInfo.Timestamp
		if currentTime.Sub(lastScheduleTime) > p.podMaxInUnschedulablePodsDuration {
			podsToMove = append(podsToMove, pInfo)
		}
	}

	if len(podsToMove) > 0 {
		// æŠŠ podInfo æ‰”åˆ° activeQ å’Œ backoffQ é˜Ÿåˆ—ä¸­.
		p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
	}
}

func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event framework.ClusterEvent) {
	activated := false
	for _, pInfo := range podInfoList {
		pod := pInfo.Pod
		if p.isPodBackingoff(pInfo) {
			// å¦‚æœè¿˜éœ€è¦ backoff é€€é¿, åˆ™æ‰”åˆ° podBackoffQ é˜Ÿåˆ—ä¸­è¿›è¡Œå»¶è¿Ÿ, åé¢ç”± flushBackoffQCompleted å¤„ç†.
			if err := p.podBackoffQ.Add(pInfo); err != nil {
			} else {
				p.unschedulablePods.delete(pod, pInfo.Gated)
			}
		} else {
			// æŠŠ podInfo æ‰”åˆ° activeQ é‡Œ, ç­‰å¾… scheduleOne è°ƒåº¦å¤„ç†.
			if added, _ := p.addToActiveQ(pInfo); added {
				activated = true
				p.unschedulablePods.delete(pod, gated)
			}
		}
	}
    ...
}
```

## bindingCycle å®ç° pod å’Œ node ç»‘å®š

`bindingCycle` ç”¨æ¥å®ç° pod å’Œ node ç»‘å®š, å…¶è¿‡ç¨‹ä¸º `prebind -> bind -> postbind`.

```go
func (sched *Scheduler) bindingCycle(
	... {

	assumedPod := assumedPodInfo.Pod

	// æ‰§è¡Œæ’ä»¶çš„ prebind é€»è¾‘
	if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() {
		return status
	}

	// æ‰§è¡Œ bind æ’ä»¶é€»è¾‘
	if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() {
		return status
	}

	// åœ¨ bind ç»‘å®šåæ‰§è¡Œæ”¶å°¾æ“ä½œ
	fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)

	return nil
}

func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) {
	defer func() {
		sched.finishBinding(fwk, assumed, targetNode, status)
	}()

	bound, err := sched.extendersBinding(assumed, targetNode)
	if bound {
		return framework.AsStatus(err)
	}
	return fwk.RunBindPlugins(ctx, state, assumed, targetNode)
}

func (sched *Scheduler) extendersBinding(pod *v1.Pod, node string) (bool, error) {
	for _, extender := range sched.Extenders {
		if !extender.IsBinder() || !extender.IsInterested(pod) {
			continue
		}
		return true, extender.Bind(&v1.Binding{
			ObjectMeta: metav1.ObjectMeta{Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID},
			Target:     v1.ObjectReference{Kind: "Node", Name: node},
		})
	}
	return false, nil
}
```

extender é»˜è®¤å°±åªæœ‰ `DefaultBinder` æ’ä»¶, è¯¥æ’ä»¶çš„ bind é€»è¾‘æ˜¯é€šè¿‡ clientset å¯¹ pod è¿›è¡Œ node ç»‘å®š..

ä»£ç ä½ç½®: `pkg/scheduler/framework/plugins/defaultbinder/default_binder.go`

```go
func (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	binding := &v1.Binding{
		ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID},
		Target:     v1.ObjectReference{Kind: "Node", Name: nodeName},
	}
	err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})
	if err != nil {
		return framework.AsStatus(err)
	}
	return nil
}
```

## æ€»ç»“

kubernetes scheduler ä» informer ç›‘å¬æ–°èµ„æºå˜åŠ¨, å½“æœ‰æ–° pod åˆ›å»ºæ—¶, scheduler éœ€è¦ä¸ºå…¶åˆ†é…ç»‘å®šä¸€ä¸ª node èŠ‚ç‚¹. åœ¨è°ƒåº¦ Pod æ—¶è¦ç»è¿‡ä¸¤ä¸ªé˜¶æ®µ, å³ è°ƒåº¦å‘¨æœŸ å’Œ ç»‘å®šå‘¨æœŸ. 

è°ƒåº¦å‘¨æœŸåˆ†ä¸ºé¢„é€‰é˜¶æ®µå’Œä¼˜é€‰é˜¶æ®µ.

- é¢„é€‰ (Predicates) æ˜¯é€šè¿‡æ’ä»¶çš„ Filter æ–¹æ³•è¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„ node èŠ‚ç‚¹
- ä¼˜é€‰ (Priorities) åˆ™å¯¹é¢„é€‰å‡ºæ¥çš„ node èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†æ’åº

ç»‘å®šå‘¨æœŸé»˜è®¤åªæœ‰ä¸€ä¸ªé»˜è®¤æ’ä»¶, ä»…ç»™ apiserver å‘èµ· node è·Ÿ pod ç»‘å®šçš„è¯·æ±‚.